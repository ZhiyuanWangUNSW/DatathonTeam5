{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzhvrixld1HF",
        "outputId": "dcab1ed4-d5dd-4967-884f-6565b9b0dc86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=8c95f2b47eac9e6b26498019464d908be868b2e3481ea917839f1740177cbdcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.2\n",
            "    Uninstalling httpx-0.27.2:\n",
            "      Successfully uninstalled httpx-0.27.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.1.143 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.54.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aGQmj56dd3D"
      },
      "outputs": [],
      "source": [
        "#%% Import Libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import unicodedata\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from googletrans import Translator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnCx8pWMdgq2",
        "outputId": "f5802b71-46fe-4cb8-a1b2-d2b2336fb9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Translator Initialization\n",
        "translator = Translator()"
      ],
      "metadata": {
        "id": "fTxEBwsxdjI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% File Paths\n",
        "input_file = '/content/drive/MyDrive/Colab Notebooks/epiwatch-latest.csv'  # Input CSV in Google Drive\n",
        "output_folder = '/content/drive/MyDrive/Colab Notebooks/processed_batches'  # Folder to save processed files\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "checkpoint_file = '/content/drive/MyDrive/Colab Notebooks/checkpoint_china.txt'  # File to store the last processed batch"
      ],
      "metadata": {
        "id": "JJJVOuW0dktE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Function to load the checkpoint\n",
        "def load_checkpoint():\n",
        "    \"\"\"Load the last processed batch index from the checkpoint file.\"\"\"\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, 'r') as file:\n",
        "            return int(file.read().strip())\n",
        "    return 0  # Default to starting from the beginning\n",
        "\n",
        "#%% Function to save the checkpoint\n",
        "def save_checkpoint(batch_index):\n",
        "    \"\"\"Save the last processed batch index to the checkpoint file.\"\"\"\n",
        "    with open(checkpoint_file, 'w') as file:\n",
        "        file.write(str(batch_index))"
      ],
      "metadata": {
        "id": "Po2vFnhIdmqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Function to clean text\n",
        "def clean_content(content):\n",
        "    \"\"\"Clean and normalize the text content.\"\"\"\n",
        "    content = unicodedata.normalize('NFKD', content)\n",
        "    content = content.replace('“', '\"').replace('”', '\"')\n",
        "    content = content.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "    content = content.replace(\"\\n\", \" \").strip()\n",
        "    return content\n",
        "\n",
        "#%% Function to fetch content with timeout\n",
        "def fetch_full_content(url, timeout=3):\n",
        "    \"\"\"Fetch content from a URL with a hard timeout.\"\"\"\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.encoding = response.apparent_encoding\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
        "            return ' '.join(paragraphs).strip() if paragraphs else \"Error: No content found\"\n",
        "        return f\"Error {response.status_code}: Unable to fetch content\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "#%% Function to translate content\n",
        "def translate_content(content):\n",
        "    \"\"\"Translate non-English content to English.\"\"\"\n",
        "    try:\n",
        "        if not content or content.startswith(\"Error\"):\n",
        "            return content  # Skip translation for errors or empty content\n",
        "        detected_lang = translator.detect(content).lang\n",
        "        if detected_lang != 'en':\n",
        "            translated = translator.translate(content, src=detected_lang, dest='en')\n",
        "            return translated.text\n",
        "        return content  # Content is already in English\n",
        "    except Exception as e:\n",
        "        return f\"Translation Error: {str(e)}\"\n",
        "\n",
        "#%% Combined function to process a row\n",
        "def process_row(row):\n",
        "    \"\"\"Fetch and translate content for a single row.\"\"\"\n",
        "    url = row['url']\n",
        "    if not isinstance(url, str) or not url.strip():\n",
        "        return \"Error: Invalid or empty URL\"\n",
        "\n",
        "    try:\n",
        "        content = fetch_full_content(url)\n",
        "        return translate_content(content)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "ojw2288qdosa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Process Data in Batches with Checkpoints\n",
        "# Load the data and filter for the country\n",
        "df = pd.read_csv(input_file)\n",
        "df = df[df['country'].isin(['China'])]  # Filter for selected country\n",
        "grouped = df.groupby('country')\n",
        "\n",
        "# Load the last processed batch index\n",
        "start_batch_index = load_checkpoint()\n",
        "print(f\"Resuming from batch index {start_batch_index}...\")\n",
        "\n",
        "# Get already processed files\n",
        "processed_batches = set(os.listdir(output_folder))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxqyuRO9drxO",
        "outputId": "74357a41-29a9-42d9-bf6f-d62f9936204b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from batch index 55...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for country, group in grouped:\n",
        "    print(f\"Processing country: {country}\")\n",
        "    group = group.reset_index(drop=True)  # Reset index for clean batching\n",
        "\n",
        "    # Process data in batches of 100\n",
        "    total_records = len(group)\n",
        "    total_batches = (total_records // 100) + (1 if total_records % 100 != 0 else 0)\n",
        "\n",
        "    for batch_index, i in enumerate(range(0, total_records, 100)):\n",
        "        # Skip already processed batches and start from the checkpoint\n",
        "        if batch_index < start_batch_index:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Generate batch file name\n",
        "            batch_file = os.path.join(output_folder, f\"{country}_batch_{batch_index + 1}.csv\")\n",
        "\n",
        "            # Skip if batch already processed\n",
        "            if os.path.basename(batch_file) in processed_batches:\n",
        "                print(f\"Skipping already processed batch: {batch_file}\")\n",
        "                continue\n",
        "\n",
        "            # Extract the current batch\n",
        "            batch = group.iloc[i:i + 100].copy()\n",
        "\n",
        "            # Process rows in the batch using multithreading\n",
        "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "                batch['Translated_Content'] = list(executor.map(process_row, [row for _, row in batch.iterrows()]))\n",
        "\n",
        "            # Save the processed batch to an individual CSV file\n",
        "            batch.to_csv(batch_file, index=False)\n",
        "            print(f\"Batch {batch_index + 1}/{total_batches} for country {country} processed and saved to {batch_file}.\")\n",
        "\n",
        "            # Save the checkpoint after processing the batch\n",
        "            save_checkpoint(batch_index)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {batch_index + 1}/{total_batches} for country {country}: {str(e)}\")\n",
        "            save_checkpoint(batch_index)  # Save checkpoint even on error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqPNzO32duuF",
        "outputId": "fbb484e5-c0b6-437d-dba8-5d648b4a443f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing country: China\n",
            "Skipping already processed batch: /content/drive/MyDrive/Colab Notebooks/processed_batches/China_batch_56.csv\n",
            "Batch 57/61 for country China processed and saved to /content/drive/MyDrive/Colab Notebooks/processed_batches/China_batch_57.csv.\n",
            "Batch 58/61 for country China processed and saved to /content/drive/MyDrive/Colab Notebooks/processed_batches/China_batch_58.csv.\n",
            "Batch 59/61 for country China processed and saved to /content/drive/MyDrive/Colab Notebooks/processed_batches/China_batch_59.csv.\n",
            "Batch 60/61 for country China processed and saved to /content/drive/MyDrive/Colab Notebooks/processed_batches/China_batch_60.csv.\n",
            "Batch 61/61 for country China processed and saved to /content/drive/MyDrive/Colab Notebooks/processed_batches/China_batch_61.csv.\n"
          ]
        }
      ]
    }
  ]
}